{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Run this cell once in Colab to install all necessary libraries\n",
    "# Install required packages for embedding, GPT, similarity search, and model loading\n",
    "!pip install transformers sentence-transformers faiss-cpu openai huggingface_hub gdown --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from google.colab import drive, files, userdata\n",
    "from huggingface_hub import login\n",
    "import shutil\n",
    "import gdown\n",
    "import requests\n",
    "from difflib import SequenceMatcher\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1_RW0O8w8aXqXD72K5hyNeI30n6GUSGFW\n",
      "From (redirected): https://drive.google.com/uc?id=1_RW0O8w8aXqXD72K5hyNeI30n6GUSGFW&confirm=t&uuid=6cc57c23-fd92-4fdb-8907-12a926ed02e6\n",
      "To: /content/ElvtrHW3_480p.zip\n",
      "100%|██████████| 300M/300M [00:02<00:00, 146MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Construct direct download URL\n",
    "url = \"https://drive.google.com/uc?id=YOUR_GOOGLE_DRIVE_FILE_ID\"\n",
    "output = \"dataset.zip\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# Unzip into target folder\n",
    "with ZipFile(output, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"unzipped_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "FORCE_DEVICE = \"auto\"  # Options: \"cpu\", \"cuda\", or \"auto\"\n",
    "BATCH_SIZE = 8\n",
    "CAPTION_PATH = \"captions_partial.json\"\n",
    "ZIP_FILE = next(iter(uploaded))  # Use the uploaded filename dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# === DEVICE SETUP ===\n",
    "device = \"cuda\" if torch.cuda.is_available() and FORCE_DEVICE != \"cpu\" else \"cpu\"\n",
    "print(f\"🚀 Using device: {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FIND IMAGE FOLDER ===\n",
    "root = \"unzipped_images\"\n",
    "subfolders = [f.path for f in os.scandir(root) if f.is_dir()]\n",
    "if not subfolders:\n",
    "    raise ValueError(\"No folders found inside unzipped folder.\")\n",
    "image_folder = subfolders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Found 3719 images\n"
     ]
    }
   ],
   "source": [
    "# === GATHER IMAGES ===\n",
    "image_files = []\n",
    "for r, _, fns in os.walk(image_folder):\n",
    "    for fn in fns:\n",
    "        if fn.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_files.append(os.path.join(r, fn))\n",
    "\n",
    "print(f\"🖼️ Found {len(image_files)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROMPT FOR HUGGING FACE TOKEN  ===\n",
    "# Make sure Notebook Access is enabled\n",
    "# Then, add your Hugging Face token using 'HF_TOKEN' as the key\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Login using the secret\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc3a98636d44190a59977602b2c89b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7e80727dea4822899705422a991489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38195f5c646a4d9095c0c3ff83dcd349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea40f31228a242b4ada7fd67fd4ca833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d0caa6d7f241da9841b70ffdb7e9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cff21e0f05461cb5ef0fbdb2c9f550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb96cb231bf4045bdd8f5763b79ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c45cd7b2eab4dfc8204e666f9fe1e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LOAD BLIP MODEL ===\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=False)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD/RESUME CAPTIONS ===\n",
    "if os.path.exists(CAPTION_PATH):\n",
    "    with open(CAPTION_PATH, \"r\") as f:\n",
    "        captions = json.load(f)\n",
    "    print(f\"🔁 Resuming from {len(captions)} captions\")\n",
    "else:\n",
    "    captions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Captioned 0/3719\n",
      "📸 Captioned 16/3719\n",
      "📸 Captioned 32/3719\n",
      "📸 Captioned 48/3719\n",
      "📸 Captioned 64/3719\n",
      "📸 Captioned 80/3719\n",
      "📸 Captioned 96/3719\n",
      "📸 Captioned 112/3719\n",
      "📸 Captioned 128/3719\n",
      "📸 Captioned 144/3719\n",
      "📸 Captioned 160/3719\n",
      "📸 Captioned 176/3719\n",
      "📸 Captioned 192/3719\n",
      "📸 Captioned 208/3719\n",
      "📸 Captioned 224/3719\n",
      "📸 Captioned 240/3719\n",
      "📸 Captioned 256/3719\n",
      "📸 Captioned 272/3719\n",
      "📸 Captioned 288/3719\n",
      "📸 Captioned 304/3719\n",
      "📸 Captioned 320/3719\n",
      "📸 Captioned 336/3719\n",
      "📸 Captioned 352/3719\n",
      "📸 Captioned 368/3719\n",
      "📸 Captioned 384/3719\n",
      "📸 Captioned 400/3719\n",
      "📸 Captioned 416/3719\n",
      "📸 Captioned 432/3719\n",
      "📸 Captioned 448/3719\n",
      "📸 Captioned 464/3719\n",
      "📸 Captioned 480/3719\n",
      "📸 Captioned 496/3719\n",
      "📸 Captioned 512/3719\n",
      "📸 Captioned 528/3719\n",
      "📸 Captioned 544/3719\n",
      "📸 Captioned 560/3719\n",
      "📸 Captioned 576/3719\n",
      "📸 Captioned 592/3719\n",
      "📸 Captioned 608/3719\n",
      "📸 Captioned 624/3719\n",
      "📸 Captioned 640/3719\n",
      "📸 Captioned 656/3719\n",
      "📸 Captioned 672/3719\n",
      "📸 Captioned 688/3719\n",
      "📸 Captioned 704/3719\n",
      "📸 Captioned 720/3719\n",
      "📸 Captioned 736/3719\n",
      "📸 Captioned 752/3719\n",
      "📸 Captioned 768/3719\n",
      "📸 Captioned 784/3719\n",
      "📸 Captioned 800/3719\n",
      "📸 Captioned 816/3719\n",
      "📸 Captioned 832/3719\n",
      "📸 Captioned 848/3719\n",
      "📸 Captioned 864/3719\n",
      "📸 Captioned 880/3719\n",
      "📸 Captioned 896/3719\n",
      "📸 Captioned 912/3719\n",
      "📸 Captioned 928/3719\n",
      "📸 Captioned 944/3719\n",
      "📸 Captioned 960/3719\n",
      "📸 Captioned 976/3719\n",
      "📸 Captioned 992/3719\n",
      "📸 Captioned 1008/3719\n",
      "📸 Captioned 1024/3719\n",
      "📸 Captioned 1040/3719\n",
      "📸 Captioned 1056/3719\n",
      "📸 Captioned 1072/3719\n",
      "📸 Captioned 1088/3719\n",
      "📸 Captioned 1104/3719\n",
      "📸 Captioned 1120/3719\n",
      "📸 Captioned 1136/3719\n",
      "📸 Captioned 1152/3719\n",
      "📸 Captioned 1168/3719\n",
      "📸 Captioned 1184/3719\n",
      "📸 Captioned 1200/3719\n",
      "📸 Captioned 1216/3719\n",
      "📸 Captioned 1232/3719\n",
      "📸 Captioned 1248/3719\n",
      "📸 Captioned 1264/3719\n",
      "📸 Captioned 1280/3719\n",
      "📸 Captioned 1296/3719\n",
      "📸 Captioned 1312/3719\n",
      "📸 Captioned 1328/3719\n",
      "📸 Captioned 1344/3719\n",
      "📸 Captioned 1360/3719\n",
      "📸 Captioned 1376/3719\n",
      "📸 Captioned 1392/3719\n",
      "📸 Captioned 1408/3719\n",
      "📸 Captioned 1424/3719\n",
      "📸 Captioned 1440/3719\n",
      "📸 Captioned 1456/3719\n",
      "📸 Captioned 1472/3719\n",
      "📸 Captioned 1488/3719\n",
      "📸 Captioned 1504/3719\n",
      "📸 Captioned 1520/3719\n",
      "📸 Captioned 1536/3719\n",
      "📸 Captioned 1552/3719\n",
      "📸 Captioned 1568/3719\n",
      "📸 Captioned 1584/3719\n",
      "📸 Captioned 1600/3719\n",
      "📸 Captioned 1616/3719\n",
      "📸 Captioned 1632/3719\n",
      "📸 Captioned 1648/3719\n",
      "📸 Captioned 1664/3719\n",
      "📸 Captioned 1680/3719\n",
      "📸 Captioned 1696/3719\n",
      "📸 Captioned 1712/3719\n",
      "📸 Captioned 1728/3719\n",
      "📸 Captioned 1744/3719\n",
      "📸 Captioned 1760/3719\n",
      "📸 Captioned 1776/3719\n",
      "📸 Captioned 1792/3719\n",
      "📸 Captioned 1808/3719\n",
      "📸 Captioned 1824/3719\n",
      "📸 Captioned 1840/3719\n",
      "📸 Captioned 1856/3719\n",
      "📸 Captioned 1872/3719\n",
      "📸 Captioned 1888/3719\n",
      "📸 Captioned 1904/3719\n",
      "📸 Captioned 1920/3719\n",
      "📸 Captioned 1936/3719\n",
      "📸 Captioned 1952/3719\n",
      "📸 Captioned 1968/3719\n",
      "📸 Captioned 1984/3719\n",
      "📸 Captioned 2000/3719\n",
      "📸 Captioned 2016/3719\n",
      "📸 Captioned 2032/3719\n",
      "📸 Captioned 2048/3719\n",
      "📸 Captioned 2064/3719\n",
      "📸 Captioned 2080/3719\n",
      "📸 Captioned 2096/3719\n",
      "📸 Captioned 2112/3719\n",
      "📸 Captioned 2128/3719\n",
      "📸 Captioned 2144/3719\n",
      "📸 Captioned 2160/3719\n",
      "📸 Captioned 2176/3719\n",
      "📸 Captioned 2192/3719\n",
      "📸 Captioned 2208/3719\n",
      "📸 Captioned 2224/3719\n",
      "📸 Captioned 2240/3719\n",
      "📸 Captioned 2256/3719\n",
      "📸 Captioned 2272/3719\n",
      "📸 Captioned 2288/3719\n",
      "📸 Captioned 2304/3719\n",
      "📸 Captioned 2320/3719\n",
      "📸 Captioned 2336/3719\n",
      "📸 Captioned 2352/3719\n",
      "📸 Captioned 2368/3719\n",
      "📸 Captioned 2384/3719\n",
      "📸 Captioned 2400/3719\n",
      "📸 Captioned 2416/3719\n",
      "📸 Captioned 2432/3719\n",
      "📸 Captioned 2448/3719\n",
      "📸 Captioned 2464/3719\n",
      "📸 Captioned 2480/3719\n",
      "📸 Captioned 2496/3719\n",
      "📸 Captioned 2512/3719\n",
      "📸 Captioned 2528/3719\n",
      "📸 Captioned 2544/3719\n",
      "📸 Captioned 2560/3719\n",
      "📸 Captioned 2576/3719\n",
      "📸 Captioned 2592/3719\n",
      "📸 Captioned 2608/3719\n",
      "📸 Captioned 2624/3719\n",
      "📸 Captioned 2640/3719\n",
      "📸 Captioned 2656/3719\n",
      "📸 Captioned 2672/3719\n",
      "📸 Captioned 2688/3719\n",
      "📸 Captioned 2704/3719\n",
      "📸 Captioned 2720/3719\n",
      "📸 Captioned 2736/3719\n",
      "📸 Captioned 2752/3719\n",
      "📸 Captioned 2768/3719\n",
      "📸 Captioned 2784/3719\n",
      "📸 Captioned 2800/3719\n",
      "📸 Captioned 2816/3719\n",
      "📸 Captioned 2832/3719\n",
      "📸 Captioned 2848/3719\n",
      "📸 Captioned 2864/3719\n",
      "📸 Captioned 2880/3719\n",
      "📸 Captioned 2896/3719\n",
      "📸 Captioned 2912/3719\n",
      "📸 Captioned 2928/3719\n",
      "📸 Captioned 2944/3719\n",
      "📸 Captioned 2960/3719\n",
      "📸 Captioned 2976/3719\n",
      "📸 Captioned 2992/3719\n",
      "📸 Captioned 3008/3719\n",
      "📸 Captioned 3024/3719\n",
      "📸 Captioned 3040/3719\n",
      "📸 Captioned 3056/3719\n",
      "📸 Captioned 3072/3719\n",
      "📸 Captioned 3088/3719\n",
      "📸 Captioned 3104/3719\n",
      "📸 Captioned 3120/3719\n",
      "📸 Captioned 3136/3719\n",
      "📸 Captioned 3152/3719\n",
      "📸 Captioned 3168/3719\n",
      "📸 Captioned 3184/3719\n",
      "📸 Captioned 3200/3719\n",
      "📸 Captioned 3216/3719\n",
      "📸 Captioned 3232/3719\n",
      "📸 Captioned 3248/3719\n",
      "📸 Captioned 3264/3719\n",
      "📸 Captioned 3280/3719\n",
      "📸 Captioned 3296/3719\n",
      "📸 Captioned 3312/3719\n",
      "📸 Captioned 3328/3719\n",
      "📸 Captioned 3344/3719\n",
      "📸 Captioned 3360/3719\n",
      "📸 Captioned 3376/3719\n",
      "📸 Captioned 3392/3719\n",
      "📸 Captioned 3408/3719\n",
      "📸 Captioned 3424/3719\n",
      "📸 Captioned 3440/3719\n",
      "📸 Captioned 3456/3719\n",
      "📸 Captioned 3472/3719\n",
      "📸 Captioned 3488/3719\n",
      "📸 Captioned 3504/3719\n",
      "📸 Captioned 3520/3719\n",
      "📸 Captioned 3536/3719\n",
      "📸 Captioned 3552/3719\n",
      "📸 Captioned 3568/3719\n",
      "📸 Captioned 3584/3719\n",
      "📸 Captioned 3600/3719\n",
      "📸 Captioned 3616/3719\n",
      "📸 Captioned 3632/3719\n",
      "📸 Captioned 3648/3719\n",
      "📸 Captioned 3664/3719\n",
      "📸 Captioned 3680/3719\n",
      "📸 Captioned 3696/3719\n",
      "📸 Captioned 3712/3719\n",
      "✅ Done captioning 3719 images\n"
     ]
    }
   ],
   "source": [
    "# === BATCH CAPTIONING ===\n",
    "def batch_caption(paths):\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in paths]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = blip_model.generate(**inputs)\n",
    "    return [processor.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "pending = [p for p in image_files if p not in captions]\n",
    "total = len(pending)\n",
    "\n",
    "for i in range(0, total, BATCH_SIZE):\n",
    "    batch_paths = pending[i:i+BATCH_SIZE]\n",
    "    try:\n",
    "        batch_captions = batch_caption(batch_paths)\n",
    "        for path, caption in zip(batch_paths, batch_captions):\n",
    "            captions[path] = caption\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed batch at {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if i % (BATCH_SIZE * 2) == 0:\n",
    "        print(f\"📸 Captioned {i}/{total}\")\n",
    "        with open(CAPTION_PATH, \"w\") as f:\n",
    "            json.dump(captions, f)\n",
    "\n",
    "with open(\"captions_final.json\", \"w\") as f:\n",
    "    json.dump(captions, f)\n",
    "\n",
    "print(f\"✅ Done captioning {len(captions)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83c03289bad49dd92bc150fea4e390c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea837e182b74f7cab92b70f1fe4fb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cc54a65c5b4203979d6c439f6f36f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f51282fd9e645a890fb46ab5cb3d08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37d5a79a7b24e14b827d869da03d11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233782c78488478e84b8c6b54c3c9e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c95284b7b6145208bb9128296b339db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95130ded013a4bdfb3304b221c12af48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeaf0232bec4c33a76e1c44c0143e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43b4b0336424a09905e9f80acd1c4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d16e84630f4d96a24ff64849b5b4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 FAISS index built\n"
     ]
    }
   ],
   "source": [
    "# === EMBED CAPTIONS ===\n",
    "texts = list(captions.values())\n",
    "try:\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedder.encode(texts)\n",
    "except:\n",
    "    embeddings = np.array([[len(t)] * 5 for t in texts])\n",
    "\n",
    "# === FAISS INDEX ===\n",
    "faiss_index = faiss.IndexFlatL2(embeddings[0].shape[0])\n",
    "faiss_index.add(np.array(embeddings))\n",
    "print(\"📦 FAISS index built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RETRIEVE MATCHES ===\n",
    "def retrieve_top_matches(query, k=5):\n",
    "    q_embed = embedder.encode([query])\n",
    "    _, idxs = faiss_index.search(np.array(q_embed), k)\n",
    "    items = list(captions.items())\n",
    "    return [(os.path.basename(items[i][0]), texts[i]) for i in idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top matches for: 'Are any cars blocking the bus lane?'\n",
      "• video1_frame_278.jpg: a road with cars and a bus on it\n",
      "• video1_frame_74.jpg: a street with cars and a bus on it\n",
      "• video1_frame_736.jpg: a street with cars and a bus on it\n",
      "• video1_frame_282.jpg: a street with cars and a bus on it\n",
      "• video1_frame_221.jpg: a street with cars and a bus on it\n"
     ]
    }
   ],
   "source": [
    "# === SAMPLE QUERY 1 (Non-Agentic)===\n",
    "query = \"Are any cars blocking the bus lane?\"\n",
    "matches = retrieve_top_matches(query)\n",
    "\n",
    "print(f\"\\n🔍 Top matches for: '{query}'\")\n",
    "for fname, cap in matches:\n",
    "    print(f\"• {fname}: {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top matches for: 'Are any cars that are double parked?'\n",
      "• video4_frame_34.jpg: two cars parked in a parking lot\n",
      "• video4_frame_46.jpg: two cars parked in a parking lot\n",
      "• video4_frame_50.jpg: two cars parked in a parking lot\n",
      "• video4_frame_33.jpg: two cars parked in a parking lot\n",
      "• video4_frame_310.jpg: two cars parked on the side of a road\n"
     ]
    }
   ],
   "source": [
    "# === SAMPLE QUERY 2 (Non-Agentic) ===\n",
    "query = \"Are any cars that are double parked?\"\n",
    "matches = retrieve_top_matches(query)\n",
    "\n",
    "print(f\"\\n🔍 Top matches for: '{query}'\")\n",
    "for fname, cap in matches:\n",
    "    print(f\"• {fname}: {cap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"captions_final.json\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
